I ended up completing the unit on Indexes in MongoDB yesterday evening. It was definitely the most challenging unit for me to understand yet. I think the reason I was struggling to wrap my head around it is because the main purpose of using an Index is to improve performance and I am not used to thinking from that perspective yet. I'll recap the details:

In MongoDB, an index is an ordered data structure that is designed to make accessing specific subsets of documents much more efficient. Instead of scanning an entire collection when a query is made, you can set up an index to locate and access documents that match a specific criteria. So for example, say you created a username index. When you make a query to find user names that match a certain value, by default mongoDB would parse through each document to check if that field matches. Scanning every document in the collection can be slow though, especially at scale. An index will store the value of the usernames in an ordered data structure like a binary tree for example that is very quick to parse through in comparison to scanning entire documents. The values that match will then be returned. So regardless of what query you are running this process of looking through an index can make dramatic improvements for performance.

I learned about single-field and compound-field indexes. Single field is as it sounds, the index just stores a single fields value. Compound would instead store a document with more than one fields. The username example above would be considered a single-field index and would be useful when it is very common to make query's based on usernames. A compound-field index would come in handy when you have a situation where you tend to make query's based on several fields together, such as looking for a person by first name and last name. In compound-field indexes each field has it's own sort order which can affect how efficiently the data is traversed. 

If an index contains an array than it is said to be a multikey index. Compound indexes can only contain one of these. Having the ability to support searching through arrays is great but it can require quite a bit more resources that eat up storage and write speeds. Which brings up an important topic!

The tradeoff of using indexes in the first place is that write speed is degraded and storage requirements are increased. This comes with the benefit of query performance so you need to strike a balance that works best for the application. Write speed is affected because indexes store many different index fields which need to be updated any time a CRUD operation affects the data that it points to. These index fields of course require storage to persist.

Creating an index is easy, just call the createIndex or createIndexes method and pass in the field(s) you want to create an index for along with each fields sort order (ascending or descending). You can view all indexes by calling getIndexes. If you want to know whether or not a specific query is using an index you can chain on the explain method to the query and you will be get to view the stages that the query takes to retreieve the data. If an index is being used it will be visible there. To delete an index just use the dropIndex method (or dropIndexes if doing several at once).

Alright so that was a summary of what I covered last night. Moving forward now to learn about Atlas Search! What I have heard so far is that it will allow me to build high-performing search functionality into my sites with ease which is kind of cool.

So far this unit has been really interesting. Turns out that there are different types of indexes. In the previous unit I had learned about database indexes where the goal is to improve querying performance. This new unit is covering search indexes. The goal of search indexes is to facilitate the use of relevance-based searches. Relevance-based searching is the operation that takes place when a user submits a query in a search-bar and the search engine tries to match all of the database results to the search term is a way that best matches the query. A score is assigned to each record indicating how relevant it is to the query and this decides the order in which the records are returned. Optimizing this must be very important for SEO i'm imagining. 

Search indexes are configured based on several variables. First of all, we need to specify which index analyzer and search analyzer is being used. This defines how searchable terms are created from the index data as well as how queries are parsed for these searchable terms. The industry standard is to use an open-source search library called Lucenda, specifically the Lucenda Standard Analyzer. This 'tokenizes' data into individual words or terms which removes punctuation and other non-alphanumeric characters, converts them to lowercase, removes non-useful stop words (such as in, the as, etc.), and applies a stemming algorithm to the terms to reduce them to their root form (run instead of running) which helps capture all of the variations. The index anlayzer handles this process for the data that is being indexed, while the search analyzer does it for the search queries. Lucenda standard is used for both of default in mongoDB but others are available.

Next you can choose between dynamic or static mapping. Dynamic mapping means that all fields in a document is indexed and is therefore searchable. This can be slow though. Static mapping allows you to define which fields are searchable which is useful if you anticipate that users will only really be searching for certain fields in your data.

Wow the remainder of the unit got pretty intense! I learned how to use the $search aggregation stage and what compound operators are, then I learned how to group search results together into buckets called facets. The search aggregation stage allows you to return documents in an aggregation pipeline using relevant search functionality. This is awesome because it allows for data analysis based on relevancy to a search term. Within the search stage you will specify which search index is being used and then use operators to decide which data is included in the returned results. The operators you can use are called must, must-not, should, and filter. Each operator will have clauses within it that have to be met. The must operator dictates that the clause must be included in the query, and the documents relevancy score is boosted if so (allowing it to be pushed up closer to the top of the list of returned results). The must-not is the opposite of that. So those two are non-negotiables. The should operator won't completely remove a document if a clause is not met, but if it does happen to be met than the relevancy score will be increased thus pushing it to the top of the pile of results. The filter operator is very much like the must operator in that if the query doesn't meet it's clauses it will be filtered out of the results. However if the document does pass through the filter it's score won't be impacted. This is to be used when it's important that information is included in document but not essential like a must clause.

Compound operators are when more than one of these operators are used within a single search stage. You can combine the must and must not operators for example to ensure that documents must match the term 'apple' but must not have the term 'fuji' in it to appear in the search results. This allows you to really fine-tune which documents are getting passed down the aggregation pipeline. The relevancy score certainly help to as it decides the order in which they appear. Having a built in search engine for your data seems really powerful.